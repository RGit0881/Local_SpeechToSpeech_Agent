# npc-local â€” Models, Performance & Licenses

This doc explains the **models** used in the pipeline, how to **swap/size** them, expected **performance**, and the **licenses** that apply.

```
Microphone (WAV)
   â”‚
   â”œâ”€â”€â–º STT â€” fasterâ€‘whisper (CTranslate2) â†’ transcript
   â”‚
   â”œâ”€â”€â–º LLM â€” Ollama model (default: gpt-oss:20b) â†’ reply text
   â”‚
   â””â”€â”€â–º TTS â€” Coqui XTTS v2 (multiâ€‘speaker cloning) â†’ WAV
```

---

## 1) Speechâ€‘toâ€‘Text (STT)

**Default stack**
- **Engine:** [fasterâ€‘whisper](https://github.com/SYSTRAN/faster-whisper) (CTranslate2 backend)
- **Model family:** OpenAI Whisper (tiny/base/small/medium/**large-v3**)
- **Env knobs**
  - `WHISPER_SIZE` â€” one of `tiny | base | small | medium | large-v3` (default: `small`)
  - `WHISPER_COMPUTE` â€” `int8` on CPU (default), `float16` on GPU
- **Characteristics**
  - Robust multilingual ASR, good accuracy from `small` upward.
  - `vad_filter=True` used to trim long silences.

**Performance guidance (rough)**
- CPU `small`: realâ€‘time or 1â€“2Ã— RT on modern laptops for short phrases.
- GPU `large-v3` w/ FP16: best accuracy; latency depends on VRAM and clip length.

**Licensing**
- Whisper models and code are **MITâ€‘licensed** (commercial use permitted).
- fasterâ€‘whisper is **MIT** as well.
- CTranslate2 runtime is permissively licensed (MIT).

---

## 2) Large Language Model (LLM)

**Default**
- **Runtime:** [Ollama](https://ollama.com/)
- **Model name:** `gpt-oss:20b` (loaded by the server via `OLLAMA_URL /api/chat`)
- **Env knobs**
  - `LLM_MODEL` â€” model tag to use (default `gpt-oss:20b`)
  - `CONTEXT_TOKENS` â€” prompt window (default `8192`)
  - `LLM_MAX_TOKENS` â€” max new tokens per reply (default `200`)
  - `LLM_TEMPERATURE` â€” creativity (default `0.7`)
  - `LLM_THREADS` â€” CPU threads (0 = auto)

> ðŸ” **Swapping models**: Any chat-capable Ollama model will work
> (e.g., `llama3.1:8b-instruct`, `qwen2.5:14b-instruct`, etc.).
> Make sure to `ollama pull <model>` first, then set `LLM_MODEL` and restart the container.

**Performance guidance (rough)**
- 20B class models run **best on GPU** with â‰¥ 12â€“16â€¯GB VRAM (quantized).
- On CPU, expect multiâ€‘second latency per short reply.
- For snappier local iteration, try `8Bâ€“14B` instruct models.

**Licensing**
- **Code (Ollama)** is permissive, but **model weights** each have their own license.
- Always check: `ollama show <model>` â†’ read the **license** and **modality/usage** notes.
- University / lab use **may** be fine; **commercial** redistribution may not. **You are responsible** for compliance.

---

## 3) Textâ€‘toâ€‘Speech (TTS)

**Default**
- **Engine:** [Coqui TTS](https://github.com/coqui-ai/TTS)
- **Model:** `tts_models/multilingual/multi-dataset/xtts_v2` (multiâ€‘speaker, crossâ€‘lingual cloning)
- **Env knobs**
  - `TTS_LANGUAGE` â€” default language (default `en`)
  - `TTS_GLOBAL_RATE` â€” global speed multiplier (default `1.0`)
- **SSML subset supported in server**
  - `<break time="500ms|1s">`
  - `<prosody rate="x-slow|slow|medium|fast|x-fast|1.2|0.8">`

**Voice reference is required**
- XTTS v2 is **multiâ€‘speaker**; you **must** supply a voice sample:
  - Perâ€‘NPC upload â†’ stored at `/data/voices/{id}/voice.wav`.
  - Or library reference via `voice_ref` mapped under `/app/voices`.

**Performance guidance**
- CPU synthesis works but is slower; **GPU (CUDA)** speeds it up significantly.
- Sample rates are 24â€¯kHz PCM; pipeline writes WAV (`PCM_16`).

**Licensing (important)**
- XTTS v2 weights are under **CPML** (Coqui Public Model License).
- **Nonâ€‘commercial use** allowed under CPML; **commercial use requires a paid license**.
- The container sets `COQUI_TOS_AGREED=1`; **you must ensure your use is compliant**.
- Review: https://coqui.ai/cpml.txt and contact Coqui for commercial licensing.

---

## 4) GPU vs CPU

- The server detects GPU at runtime. If not visible, it falls back to CPU:
  - STT switches to `int8` CTranslate2 compute.
  - TTS and LLM continue on CPU (slower).
- To enable GPU in Docker:
  1. Install **NVIDIA drivers**, **CUDA**, and **NVIDIA Container Toolkit**.
  2. Use the GPU override compose file:  
     `docker compose -f docker-compose.yml -f docker-compose.gpu.yml up -d --build`
  3. Verify: `docker run --rm --gpus=all nvidia/cuda:12.5.0-base-ubuntu24.04 nvidia-smi`

---

## 5) Model Sizing & Tips

- **Whisper**: Start with `small` for speed, upgrade to `medium`/`large-v3` if accuracy is insufficient.
- **LLM**: Prefer `8Bâ€“14B` models during development. Raise to `20B+` when youâ€™re satisfied with latency.
- **Quantization**: Ollama models are often quantized by default (e.g., `Q4_K_M`) to reduce VRAM need.
- **Threads**: If the CPU is oversubscribed, cap `LLM_THREADS` to `physical_cores` for stability.
- **Context**: Keep prompts short, use `HIST_MAX_TURNS` to limit chat memory (env in server).

---

## 6) Legal & Ethics Checklist

- âœ… **Consent for voice cloning**: Only use a speaker sample if you have **recorded it yourself** or have **explicit, written permission** from the owner. Some jurisdictions consider voice a biometric identifier.
- âœ… **Attribution**: If your lab requires it, cite Whisper, fasterâ€‘whisper, Coqui TTS, and the specific LLM.
- âœ… **Redistribution**: Do **not** redistribute proprietary weights. Point collaborators to pull from official sources.
- âœ… **University projects**: Nonâ€‘commercial **does not automatically** equal license complianceâ€”read each model license carefully.
- âœ… **Data handling**: Audio files are stored locally under `/data/voices`. The server doesnâ€™t exfiltrate data, but follow your IRB/IT policies.

---

## 7) Quick Switch Matrix

| Layer | Env var(s) | Common values | Notes |
|------:|------------|---------------|-------|
| STT | `WHISPER_SIZE`, `WHISPER_COMPUTE` | `small`, `medium`, `large-v3` / `int8`, `float16` | `float16` requires CUDA |
| LLM | `LLM_MODEL`, `CONTEXT_TOKENS`, `LLM_MAX_TOKENS`, `LLM_TEMPERATURE`, `LLM_THREADS` | `gpt-oss:20b`, `llama3.1:8b-instruct`, `qwen2.5:14b-instruct` | `ollama pull` first |
| TTS | `TTS_LANGUAGE`, `TTS_GLOBAL_RATE` | `en`, `es`, â€¦ / `0.8â€“1.4` | Provide `voice_wav` or `voice_ref` |

---

## 8) Credits

- **OpenAI Whisper** (MIT) â€” ASR models and tokenizer.
- **fasterâ€‘whisper** (MIT) â€” Efficient Whisper inference (CTranslate2).
- **Ollama** â€” Local model runtime with simple HTTP API.
- **Coqui TTS** â€” XTTS v2 multiâ€‘speaker TTS.

